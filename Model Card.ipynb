{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57de8fe9-9f18-4322-8b0c-941606b08ecc",
   "metadata": {},
   "source": [
    "# **Model Card**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9862f9e-3f7e-4a2b-963d-42ffe12062d2",
   "metadata": {},
   "source": [
    "### **Model Description**\n",
    "\n",
    "<u>**Inputs**</u>\n",
    "\n",
    "The data being used throughout the course of this customer prediction problem, consists of the following columns:\n",
    "* **Patient_Number** A Unique identifier for each of the customers\n",
    "* **Age** Age of the customer\n",
    "* **Gender** Gender of the customer (Male or Female)\n",
    "* **Operation_Type** The Operation Type of the customer (Colostomy/Ileostomy/Urostomy/Nephrostomy/Incontinence/Catheter or Other)\n",
    "* **Location** Location of the patient by territory (SC, NI, NW, NE, SE1, SE2, SW, CT, ET)\n",
    "* **Product_QI** Whether the patient has raised a product related QI in the last 12 months\n",
    "* **Service_QI** Whether the patient has raised a service related QI in the last 12 months\n",
    "* **Days_since_last_order** How many days have passed since the patient last ordered\n",
    "* **Churn** A binary indicator (1 or 0) representing whether the patient has churned (1) or not (0)\n",
    "\n",
    "<u>**Outputs**</u>\n",
    "\n",
    "The model predicts a Binary outcome of whether the patient has churned (1) or not (0). \n",
    "\n",
    "<u>**Model Architecture**</u>\n",
    "\n",
    "After trialling various model architectures, the final model chosen was the 'XGBoost Classifier', which is an ensemble learning model based upon the use of Decision Trees. As opposed to the traditional random forests, where the trees are built independently, XGBoost creates such trees sequentially, with each of them having a unique focus on the mistakes that had been made from the previous tree built. The iterative approach that takes place, also accompanies gradient boosting, which essentially means that the algorithm is continuously fine-tuning the model's predictions in the direction that reduces the error most effectively. \n",
    "\n",
    "The reason for XGBoost being chosen comes down to its performance relative to other models. The reason this happens is due to features such as regularization which help to prevent overfitting. By iteratively combining and enhancing the trees at each step, a robust model is constructed that effectively predicts patient churn. As a result, XGBoost Classifier is the most lucrative choice for problems involving complex patterns and non-linear relationships in the dataset, such as the one explored. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86668bbb-f748-495a-a3b3-0a1db38d9792",
   "metadata": {},
   "source": [
    "### **Performance**\n",
    "\n",
    "<u>**Give a summary graph or metrics of how the model performs.**</u>\n",
    "\n",
    "| Dataset | Accuracy | Precision | Recall | F1-Score |\r\n",
    "|---|---|---|---|---|\r\n",
    "| Train | 0.939552 | 0.941537 | 0.939498 | 0.939479 |\r\n",
    "| Test | 0.907249 | 0.912523 | 0.907465 | 0.906983\n",
    "\n",
    "\n",
    "From the above, we observe that the model as a whole achieves reasonably good performance upon both the training and test sets - therefore indicating that the model is performing well to unseen data.\r\n",
    "\r\n",
    "We also observe that the accuracy is slightly lower on the test set compared to the training set, which can be expected, since models often perform better on the training set as opposed to the test set.\r\n",
    "\r\n",
    "Precision is slightly lower on the test set, this suggesting that the model may be making more false positives predictions on the unseen data.\r\n",
    "\r\n",
    "Recaalso slightly lower on the g and test set - therefore the model is capturing the majority of the positive instancs.\r\n",
    "\r\n",
    "F-1 Sagain core is slightly lower on the test set, but still relatively high, and so can be accepted. |   |    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0c5048-4aa6-4477-be93-59363f0b09f1",
   "metadata": {},
   "source": [
    "## **Limitations**\n",
    "\n",
    "**<u>Outline the limitations of your model</u>**\n",
    "\n",
    "Despite the XGBoost Classifier model being a powerful tool for the prediction of patient churn, we should appreciate that this does leverage some limitations, such as:\n",
    "* **Data Dependence** The model's performance relies heavily on the data being of high quality and representative of the wider population. If at any chance, bias iss introduced or some of the features are missing (particularly age and days since last order), then the model's accuracy in predicting patient churn may be harnessed.\n",
    "* **Interpretability** The model also provides some knowledge into the importance of each of the features, therefore allowing us to understand what contributes more or less to a patient churning. However, it is not possible to fully understand the intricate relationships between the features that contribute to patient churn in a user-friendly manner. As a result, this can be challenging for when presenting the findings to others.\n",
    "* **Feature Engineering** In order for the model to perform exceptionally, the features need to be engineered, else the model's performance may be compromised."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c319190-19cd-4f70-bf4f-2c57b2377548",
   "metadata": {},
   "source": [
    "## **Trade-Offs**\n",
    "\n",
    "**<u>Outline any trade-offs of your model, such as any circumstances where the model exhibits performance issues.</u>**\n",
    "\n",
    "Although a powerful model in the prediction of patient churn, it also comes with many caveats that may hinder the performance. In particular:\n",
    "* **Compuational Complexity** The model can be quite computationally expensive, particularly for large datasets or with a large number of features. This is due to the sequential nature of the construction of the trees, in comparison to more simpler models such as logistic regression or random forests. As well as this, a significant amount of memory is also required in dealing with such large datasets. Thankfully, as mentioned in the datasheet, the data in question is relatively small, however if the dispensers market share grows drastically, then this could become a problem to consider.\n",
    "* **Overfitting** XGBoost Classifier is prone to overfitting; even with the employment of regularization mechanisms.\n",
    "* **Hyperparameter Tuning** The correct tuning of the hyperparameters (tree depth, learning rate etc.) is of high importance, as it can otherwise result in underperformance. This therefore, careful tuning using methods such as Grid Search or Random Search is highly necessary. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
